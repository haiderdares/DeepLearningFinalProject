{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 442057,
          "sourceType": "datasetVersion",
          "datasetId": 161598
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haiderdares/DeepLearningFinalProject/blob/main/sp25_dargupshe_data02_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìùIntroduction & Dataset Overview"
      ],
      "metadata": {
        "id": "f5Td1wiPN_4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Problem Statement\n",
        "**Autonomous driving systems rely on deep learning-based object detection models to identify vehicles, pedestrians, traffic signs, and signals in real-time. These models are built using Convolutional Neural Networks (CNNs) and advanced architectures like YOLO (You Only Look Once) to process road scene images and make accurate driving decisions.**\n",
        "\n",
        "**In this project, we focus on training a deep learning-based object detection model using the Berkeley DeepDrive (BDD100K) dataset, one of the largest and most diverse self-driving datasets. By leveraging YOLOv8, a state-of-the-art deep learning model, we aim to develop a system that can efficiently detect multiple objects in complex driving environments.**"
      ],
      "metadata": {
        "id": "itPqgflFN_4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Dataset Description"
      ],
      "metadata": {
        "id": "yzF-LTkuN_4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The BDD100K dataset consists of:**\n",
        "\n",
        "**100,000 driving scenes captured from different locations, times of day, and weather conditions.**\n",
        "\n",
        "**Annotated bounding boxes for:**\n",
        "\n",
        "**Cars, pedestrians, traffic signs, traffic lights, bicycles, motorcycles, and more.**\n",
        "\n",
        "**Metadata: Additional labels such as road conditions, time of day, and weather.**\n",
        "\n",
        "**For this project, we use a subset of BDD100K containing annotated bounding boxes for object detection.**"
      ],
      "metadata": {
        "id": "QDjizf6EN_4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Machine Learning Problem: Object Detection"
      ],
      "metadata": {
        "id": "tpgLQSZQN_4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This project addresses an object detection problem, where the goal is to:**\n",
        "\n",
        "**Detect objects in self-driving car images.**\n",
        "\n",
        "**Classify objects (e.g., car, person, traffic sign).**\n",
        "\n",
        "**Localize objects using bounding boxes.**"
      ],
      "metadata": {
        "id": "sgCwb_wxN_4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "6HSbYgc_N_4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# List all datasets in the Kaggle input directory\n",
        "print(\"Available datasets in /kaggle/input/:\")\n",
        "print(os.listdir(\"/kaggle/input/\"))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WUUmPkoQN_4e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_path = \"/kaggle/input/solesensei_bdd100k/\"\n",
        "\n",
        "# List contents\n",
        "print(\"Contents of the dataset folder:\")\n",
        "print(os.listdir(dataset_path))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "qmPGGTcdN_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "bdd100k_path = os.path.join(dataset_path, \"bdd100k\")  # Check inside 'bdd100k'\n",
        "print(\"Contents of 'bdd100k' folder:\")\n",
        "print(os.listdir(bdd100k_path))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OVbkZjZpN_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define path to second 'bdd100k' directory\n",
        "bdd100k_deep_path = os.path.join(bdd100k_path, \"bdd100k\")  # Nested bdd100k\n",
        "print(\"Contents of the second 'bdd100k' folder:\")\n",
        "print(os.listdir(bdd100k_deep_path))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "LCpD0ASlN_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = os.path.join(bdd100k_deep_path, \"images\")\n",
        "\n",
        "if os.path.exists(image_folder):\n",
        "    print(\"Images folder found! Listing its contents:\")\n",
        "    print(os.listdir(image_folder))\n",
        "else:\n",
        "    print(\"No 'images' folder found. Check dataset structure.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YlT0npcON_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Contents of '100k' folder:\", os.listdir(os.path.join(image_folder, \"100k\")))\n",
        "print(\"Contents of '10k' folder:\", os.listdir(os.path.join(image_folder, \"10k\")))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-q2H6PkWN_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_folder = os.path.join(image_folder, \"100k\", \"train\")  # Change to \"10k\" if needed\n",
        "\n",
        "# Verify image files exist\n",
        "print(\"Sample images in train folder:\", os.listdir(train_image_folder)[:5])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-nnAXhxON_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select a sample image\n",
        "sample_image_path = os.path.join(train_image_folder, os.listdir(train_image_folder)[0])\n",
        "\n",
        "# Load and display the image\n",
        "img = cv2.imread(sample_image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Sample Image from BDD100K\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yUUKCKnIN_4f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = \"/kaggle/input/solesensei_bdd100k/bdd100k/bdd100k/images/100k/train/\"\n",
        "val_dataset_path = \"/kaggle/input/solesensei_bdd100k/bdd100k/bdd100k/images/100k/val/\"\n",
        "test_dataset_path = \"/kaggle/input/solesensei_bdd100k/bdd100k/bdd100k/images/100k/test/\"\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "2L2uxmRCN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the path to annotations (Check inside labels folder)\n",
        "label_folder = os.path.join(dataset_path, \"bdd100k_labels_release\")\n",
        "print(\"Contents of 'bdd100k_labels_release':\", os.listdir(label_folder))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZBmRLfJBN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "label_subfolder = os.path.join(label_folder, \"bdd100k\")  # Navigate deeper\n",
        "print(\"Contents of 'bdd100k_labels_release/bdd100k':\", os.listdir(label_subfolder))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HnzqFmpNN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "labels_path = os.path.join(label_subfolder, \"labels\")  # Navigate into labels folder\n",
        "print(\"Contents of 'bdd100k_labels_release/bdd100k/labels':\", os.listdir(labels_path))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "3k2cbefoN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "lables"
      ],
      "metadata": {
        "id": "4MDzRzQvN_4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the path to the training annotation file\n",
        "train_annotation_file = os.path.join(labels_path, \"bdd100k_labels_images_train.json\")\n",
        "\n",
        "# Load JSON file\n",
        "with open(train_annotation_file, \"r\") as f:\n",
        "    train_annotations = json.load(f)\n",
        "\n",
        "# Display first annotation sample\n",
        "print(json.dumps(train_annotations[0], indent=4))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "vWdDYmyCN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract and Display Bounding Box Information**"
      ],
      "metadata": {
        "id": "VM5Q3OhaN_4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract first image annotation\n",
        "first_annotation = train_annotations[0]\n",
        "\n",
        "# Print image name\n",
        "print(\"Image Name:\", first_annotation[\"name\"])\n",
        "\n",
        "# Print image attributes (weather, scene, time of day)\n",
        "print(\"Attributes:\", first_annotation[\"attributes\"])\n",
        "\n",
        "# Print bounding boxes for objects\n",
        "print(\"\\nDetected Objects:\")\n",
        "for obj in first_annotation[\"labels\"]:\n",
        "    if \"box2d\" in obj:  # Ensure it's a valid bounding box annotation\n",
        "        category = obj[\"category\"]\n",
        "        bbox = obj[\"box2d\"]\n",
        "        print(f\"Object: {category}, Bounding Box: {bbox}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "p9YjXIZON_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Display the Image with Bounding Boxes**"
      ],
      "metadata": {
        "id": "rlgnZtH-N_4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find a Matching Image Filename**"
      ],
      "metadata": {
        "id": "y-k4e8PWN_4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all dataset filenames\n",
        "dataset_filenames = set(os.listdir(image_folder))  # Convert to set for faster lookup\n",
        "\n",
        "# Check if annotation filenames exist in the dataset\n",
        "matched_files = [fname for fname in train_annotations if fname[\"name\"] in dataset_filenames]\n",
        "\n",
        "print(f\" Matched {len(matched_files)} images directly from annotations.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "y1ffD5uGN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import get_close_matches\n",
        "\n",
        "# Get all dataset image filenames\n",
        "dataset_filenames = list(os.listdir(image_folder))\n",
        "\n",
        "# Try to find the closest filename match for each annotation\n",
        "matched_images = {}\n",
        "\n",
        "for annotation in train_annotations[:10]:  # Limit for testing\n",
        "    annotation_name = annotation[\"name\"]\n",
        "    matched = get_close_matches(annotation_name, dataset_filenames, n=1, cutoff=0.4)\n",
        "    matched_images[annotation_name] = matched[0] if matched else \"No Match\"\n",
        "\n",
        "# Display 5 sample matches\n",
        "for annotation, matched in list(matched_images.items())[:5]:\n",
        "    print(f\"Annotation: {annotation} ‚Üí Matched: {matched}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "T_90j-BUN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load & Display an Image with Bounding Boxes**"
      ],
      "metadata": {
        "id": "AgdM0k2AN_4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select an annotation and its matched image\n",
        "sample_annotation_name = list(matched_images.keys())[0]  # First annotation name\n",
        "matched_image_name = matched_images[sample_annotation_name]  # Corresponding dataset image\n",
        "\n",
        "# Define full image path\n",
        "image_path = os.path.join(image_folder, matched_image_name)\n",
        "\n",
        "# Load and display the image\n",
        "img = cv2.imread(image_path)\n",
        "if img is not None:\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Find corresponding annotation\n",
        "    annotation_data = next(item for item in train_annotations if item[\"name\"] == sample_annotation_name)\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for obj in annotation_data[\"labels\"]:\n",
        "        if \"box2d\" in obj:\n",
        "            category = obj[\"category\"]\n",
        "            bbox = obj[\"box2d\"]\n",
        "            x1, y1, x2, y2 = int(bbox[\"x1\"]), int(bbox[\"y1\"]), int(bbox[\"x2\"]), int(bbox[\"y2\"])\n",
        "\n",
        "            # Draw rectangle\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(img, category, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the image\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Bounding Boxes for {matched_image_name}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\" Error: Unable to load image {matched_image_name}. Check dataset paths.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "b_al98DxN_4g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "99o-olb5N_4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Handle Missing Data & Corrupted Images"
      ],
      "metadata": {
        "id": "F2gr3X5GN_4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 - To Detect and Remove Corrupt Images**"
      ],
      "metadata": {
        "id": "V4E89WnWN_4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize counters\n",
        "corrupt_images = []\n",
        "\n",
        "# Check all images in the dataset\n",
        "for img_name in tqdm(os.listdir(image_folder)):\n",
        "    img_path = os.path.join(image_folder, img_name)\n",
        "\n",
        "    # Try to open the image\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # If image is None, it is likely corrupted\n",
        "    if img is None:\n",
        "        print(f\" Corrupt image detected: {img_name}\")\n",
        "        corrupt_images.append(img_path)\n",
        "        os.remove(img_path)  # Delete the corrupt image\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n Removed {len(corrupt_images)} corrupt images.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0gaM32pkN_4h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error \"OSError: [Errno 30] Read-only file system\" means that Kaggle does not allow deleting files from /kaggle/input/.\n",
        "\n",
        "As we cannot delete the data from Kaggle dataset, instead of deleting we are skipping it\n",
        "\n",
        "Since we cannot delete files, we will only log corrupt images and ignore them during training."
      ],
      "metadata": {
        "id": "Syn3dDBKN_4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize counters\n",
        "corrupt_images = []\n",
        "\n",
        "# Check all images in the dataset\n",
        "for img_name in tqdm(os.listdir(image_folder)):\n",
        "    img_path = os.path.join(image_folder, img_name)\n",
        "\n",
        "    # Try to open the image\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # If image is None, it is likely corrupted\n",
        "    if img is None:\n",
        "        print(f\"‚ùå Corrupt image detected: {img_name}\")\n",
        "        corrupt_images.append(img_name)\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n‚ö†Ô∏è Found {len(corrupt_images)} corrupt images. Skipping them during training.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "eS-tEjR4N_4h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 - Check if all images have corresponding annotation files**"
      ],
      "metadata": {
        "id": "d_x3j70WN_4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "image_filenames = set(os.listdir(image_folder))  # All image filenames\n",
        "annotation_filenames = set([ann[\"name\"] for ann in train_annotations])  # All annotated image names\n",
        "\n",
        "# Find images without annotations\n",
        "missing_annotations = image_filenames - annotation_filenames\n",
        "\n",
        "# Find annotations without corresponding images\n",
        "missing_images = annotation_filenames - image_filenames\n",
        "\n",
        "# Print results\n",
        "print(f\"‚úÖ Total Images: {len(image_filenames)}\")\n",
        "print(f\"‚úÖ Total Annotated Images: {len(annotation_filenames)}\")\n",
        "print(f\"‚ùå Images Missing Annotations: {len(missing_annotations)}\")\n",
        "print(f\"‚ùå Annotations Without Images: {len(missing_images)}\")\n",
        "\n",
        "# Display sample missing files\n",
        "print(\"\\nüìå Sample Missing Annotations:\", list(missing_annotations)[:5])\n",
        "print(\"üìå Sample Missing Images:\", list(missing_images)[:5])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e5iM_z9SN_4h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out annotations that don't have corresponding images\n",
        "filtered_annotations = [ann for ann in train_annotations if ann[\"name\"] in image_filenames]\n",
        "\n",
        "# Print summary\n",
        "print(f\"‚úÖ Filtered Annotations: {len(filtered_annotations)} (Only keeping annotations with images)\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Po5hkIU9N_4h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Convert Annotations to YOLO Format"
      ],
      "metadata": {
        "id": "FDOyU0GTN_4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we will convert BDD100K annotations from JSON format to YOLO .txt format.**"
      ],
      "metadata": {
        "id": "9QzHz6x7N_4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define class mappings (modify if needed)\n",
        "class_mapping = {\n",
        "    \"car\": 0,\n",
        "    \"person\": 1,\n",
        "    \"bike\": 2,\n",
        "    \"traffic sign\": 3,\n",
        "    \"traffic light\": 4\n",
        "}\n",
        "\n",
        "# Define the output label directory\n",
        "yolo_label_dir = \"/kaggle/working/yolo_labels/\"\n",
        "os.makedirs(yolo_label_dir, exist_ok=True)\n",
        "\n",
        "# Convert each annotation\n",
        "for annotation in filtered_annotations:\n",
        "    img_name = annotation[\"name\"]\n",
        "    yolo_label_path = os.path.join(yolo_label_dir, img_name.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    with open(yolo_label_path, \"w\") as f:\n",
        "        for obj in annotation[\"labels\"]:\n",
        "            if \"box2d\" in obj and obj[\"category\"] in class_mapping:\n",
        "                x1, y1 = obj[\"box2d\"][\"x1\"], obj[\"box2d\"][\"y1\"]\n",
        "                x2, y2 = obj[\"box2d\"][\"x2\"], obj[\"box2d\"][\"y2\"]\n",
        "\n",
        "                # Normalize for YOLO format\n",
        "                x_center = ((x1 + x2) / 2) / 1280  # Assuming image width = 1280\n",
        "                y_center = ((y1 + y2) / 2) / 720   # Assuming image height = 720\n",
        "                width = (x2 - x1) / 1280\n",
        "                height = (y2 - y1) / 720\n",
        "\n",
        "                # Write YOLO format\n",
        "                f.write(f\"{class_mapping[obj['category']]} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "print(f\" YOLO annotations saved in: {yolo_label_dir}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "h70bBEaEN_4i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify YOLO Annotations**"
      ],
      "metadata": {
        "id": "eJlt5OvLN_4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List generated YOLO label files\n",
        "yolo_files = os.listdir(yolo_label_dir)\n",
        "\n",
        "# Print total files created\n",
        "print(f\"‚úÖ Total YOLO label files created: {len(yolo_files)}\")\n",
        "\n",
        "# Display a sample YOLO label file\n",
        "sample_yolo_file = os.path.join(yolo_label_dir, yolo_files[0])  # Select first file\n",
        "print(f\"\\nüìå Sample YOLO Annotation File: {sample_yolo_file}\\n\")\n",
        "\n",
        "# Read and display contents of the sample file\n",
        "with open(sample_yolo_file, \"r\") as f:\n",
        "    yolo_data = f.readlines()\n",
        "\n",
        "print(\"üîπ YOLO Format Annotations:\")\n",
        "for line in yolo_data:\n",
        "    print(line.strip())  # Remove extra spaces\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OmCXz75WN_4i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class ID (e.g., 4 for traffic light, 0 for car)\n",
        "Normalized bounding box coordinates (x_center, y_center, width, height)"
      ],
      "metadata": {
        "id": "b039zK8RN_4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Resize & Normalize Images for YOLO Training"
      ],
      "metadata": {
        "id": "rM6Gz3W6N_4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resize all images to 640x640 (YOLO standard input size)**\n",
        "\n",
        "**Normalize images (convert pixel values to a standard format)**\n",
        "\n",
        "**Save preprocessed images for training.**"
      ],
      "metadata": {
        "id": "cMzHYFQNN_4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. To Resize & Save Images"
      ],
      "metadata": {
        "id": "7Z5jCqVeN_4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the output folder for resized images\n",
        "resized_dir = \"/kaggle/working/resized_images/\"\n",
        "os.makedirs(resized_dir, exist_ok=True)\n",
        "\n",
        "# Define target size\n",
        "TARGET_SIZE = (640, 640)  # Standard for YOLOv8\n",
        "\n",
        "# Process each image\n",
        "for img_name in tqdm(os.listdir(image_folder)):\n",
        "    img_path = os.path.join(image_folder, img_name)\n",
        "    output_path = os.path.join(resized_dir, img_name)\n",
        "\n",
        "    # Read the image\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"‚ùå Skipping corrupted image: {img_name}\")\n",
        "        continue  # Skip corrupt images\n",
        "\n",
        "    # Resize image\n",
        "    img_resized = cv2.resize(img, TARGET_SIZE)\n",
        "\n",
        "    # Save resized image\n",
        "    cv2.imwrite(output_path, img_resized)\n",
        "\n",
        "print(f\"\\n‚úÖ Resized images saved in: {resized_dir}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OT9VIDLsN_4j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify Resized Images**"
      ],
      "metadata": {
        "id": "6OJhHsHtN_4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Select a random resized image\n",
        "sample_img_name = random.choice(os.listdir(resized_dir))\n",
        "sample_img_path = os.path.join(resized_dir, sample_img_name)\n",
        "\n",
        "# Load the image\n",
        "img = cv2.imread(sample_img_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Sample Resized Image: {sample_img_name}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-sIBQSxoN_4j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Data Augmentation"
      ],
      "metadata": {
        "id": "GvbpJaBpN_4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install & Import Augmentation Library**"
      ],
      "metadata": {
        "id": "KQZ6otPUN_4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VHBBdx8yN_4j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define augmentation pipeline\n",
        "augmentation = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),  # 50% chance to flip image\n",
        "    A.RandomRotate90(p=0.3),  # Random 90-degree rotation\n",
        "    A.RandomBrightnessContrast(p=0.3),  # Adjust brightness & contrast\n",
        "    A.MotionBlur(p=0.2, blur_limit=5),  # Apply motion blur\n",
        "    A.RandomScale(scale_limit=0.2, p=0.3)  # Random zoom in/out\n",
        "], bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"category_ids\"]))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "8jd2mLAzN_4j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply Augmentations & Save New Images**"
      ],
      "metadata": {
        "id": "odEu12xoN_4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output folder for augmented images\n",
        "augmented_images_dir = \"/kaggle/working/augmented_images/\"\n",
        "augmented_labels_dir = \"/kaggle/working/augmented_labels/\"\n",
        "os.makedirs(augmented_images_dir, exist_ok=True)\n",
        "os.makedirs(augmented_labels_dir, exist_ok=True)\n",
        "\n",
        "# Process each image and apply augmentation\n",
        "for img_name in tqdm(os.listdir(resized_dir)):\n",
        "    img_path = os.path.join(resized_dir, img_name)\n",
        "    label_path = os.path.join(yolo_label_dir, img_name.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "    # Load image\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        print(f\"‚ùå Skipping corrupt image: {img_name}\")\n",
        "        continue\n",
        "\n",
        "    # Load YOLO labels\n",
        "    if not os.path.exists(label_path):\n",
        "        print(f\"‚ö†Ô∏è No annotation file found for: {img_name}\")\n",
        "        continue\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        labels = f.readlines()\n",
        "\n",
        "    # Convert bounding boxes to YOLO format for Albumentations\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for label in labels:\n",
        "        class_id, x_center, y_center, width, height = map(float, label.strip().split())\n",
        "        bboxes.append([x_center, y_center, width, height])\n",
        "        category_ids.append(class_id)\n",
        "\n",
        "    # Apply augmentation\n",
        "    augmented = augmentation(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # Save augmented image\n",
        "    aug_img_name = f\"aug_{img_name}\"\n",
        "    aug_img_path = os.path.join(augmented_images_dir, aug_img_name)\n",
        "    cv2.imwrite(aug_img_path, augmented[\"image\"])\n",
        "\n",
        "    # Save new YOLO label file\n",
        "    aug_label_path = os.path.join(augmented_labels_dir, aug_img_name.replace(\".jpg\", \".txt\"))\n",
        "    with open(aug_label_path, \"w\") as f:\n",
        "        for bbox, class_id in zip(augmented[\"bboxes\"], augmented[\"category_ids\"]):\n",
        "            f.write(f\"{int(class_id)} {' '.join(map(str, bbox))}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Data Augmentation Completed! Augmented images saved.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jBHTuos2N_4j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify Augmented Images**"
      ],
      "metadata": {
        "id": "S1md-i_1N_4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select a random augmented image\n",
        "aug_sample = random.choice(os.listdir(augmented_images_dir))\n",
        "aug_sample_path = os.path.join(augmented_images_dir, aug_sample)\n",
        "\n",
        "# Load and display the image\n",
        "aug_img = cv2.imread(aug_sample_path)\n",
        "aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(aug_img)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Sample Augmented Image: {aug_sample}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QNtkVEZaN_4j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Split Data into Train, Validation & Test Sets"
      ],
      "metadata": {
        "id": "ULwepoM-N_4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to split the augmented dataset into:\n",
        "\n",
        "80% ‚Üí Training (train/)\n",
        "10% ‚Üí Validation (val/)\n",
        "10% ‚Üí Testing (test/)"
      ],
      "metadata": {
        "id": "2GijaY_yN_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define split ratios\n",
        "train_ratio = 0.80\n",
        "val_ratio = 0.10\n",
        "test_ratio = 0.10\n",
        "\n",
        "# Define paths for train, val, test folders\n",
        "split_dirs = {\n",
        "    \"train\": \"/kaggle/working/final_dataset/train/\",\n",
        "    \"val\": \"/kaggle/working/final_dataset/val/\",\n",
        "    \"test\": \"/kaggle/working/final_dataset/test/\"\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "for split, path in split_dirs.items():\n",
        "    os.makedirs(path + \"images/\", exist_ok=True)\n",
        "    os.makedirs(path + \"labels/\", exist_ok=True)\n",
        "\n",
        "# Get list of all augmented images\n",
        "all_aug_images = os.listdir(augmented_images_dir)\n",
        "random.shuffle(all_aug_images)  # Shuffle for randomness\n",
        "\n",
        "# Split data\n",
        "train_split = int(len(all_aug_images) * train_ratio)\n",
        "val_split = int(len(all_aug_images) * (train_ratio + val_ratio))\n",
        "\n",
        "train_images = all_aug_images[:train_split]\n",
        "val_images = all_aug_images[train_split:val_split]\n",
        "test_images = all_aug_images[val_split:]\n",
        "\n",
        "# Function to move images & labels\n",
        "def move_files(image_list, split_type):\n",
        "    for img_name in image_list:\n",
        "        src_img = os.path.join(augmented_images_dir, img_name)\n",
        "        dest_img = os.path.join(split_dirs[split_type] + \"images/\", img_name)\n",
        "\n",
        "        src_label = os.path.join(augmented_labels_dir, img_name.replace(\".jpg\", \".txt\"))\n",
        "        dest_label = os.path.join(split_dirs[split_type] + \"labels/\", img_name.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "        shutil.copy(src_img, dest_img)  # Copy image\n",
        "        if os.path.exists(src_label):  # Copy label only if it exists\n",
        "            shutil.copy(src_label, dest_label)\n",
        "\n",
        "# Move files to respective folders\n",
        "move_files(train_images, \"train\")\n",
        "move_files(val_images, \"val\")\n",
        "move_files(test_images, \"test\")\n",
        "\n",
        "print(f\"‚úÖ Data split completed!\")\n",
        "print(f\"üìå Training Images: {len(train_images)}\")\n",
        "print(f\"üìå Validation Images: {len(val_images)}\")\n",
        "print(f\"üìå Testing Images: {len(test_images)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dscY9fk8N_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìå Training set sample images:\", os.listdir(split_dirs[\"train\"] + \"images/\")[:5])\n",
        "print(\"üìå Validation set sample images:\", os.listdir(split_dirs[\"val\"] + \"images/\")[:5])\n",
        "print(\"üìå Testing set sample images:\", os.listdir(split_dirs[\"test\"] + \"images/\")[:5])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YZ5nXDI-N_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6.Convert Dataset into Tensor Format"
      ],
      "metadata": {
        "id": "RP4zmFDmN_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jbddDONaN_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "diMiLJGdN_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a PyTorch Dataset Class**"
      ],
      "metadata": {
        "id": "UjvwWlquN_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLODataset(Dataset):\n",
        "    def __init__(self, image_dir, label_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.image_filenames[index]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Convert image to tensor\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Load YOLO labels\n",
        "        bboxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    class_id, x_center, y_center, width, height = map(float, line.strip().split())\n",
        "                    bboxes.append([class_id, x_center, y_center, width, height])\n",
        "\n",
        "        # Convert bounding boxes to tensor\n",
        "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
        "\n",
        "        return image, bboxes\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Cii_J21HN_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Image Transformations**"
      ],
      "metadata": {
        "id": "aqRLhFSlN_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((640, 640)),\n",
        "    transforms.ToTensor(),  # This Converts the image to PyTorch tensor (0-1 range)\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "oYwTDwSWN_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Preprocessed data"
      ],
      "metadata": {
        "id": "PiWobV4IN_4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Dataset & DataLoader**"
      ],
      "metadata": {
        "id": "Oyspx03cN_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset paths (training set)\n",
        "train_images_dir = \"/kaggle/working/final_dataset/train/images/\"\n",
        "train_labels_dir = \"/kaggle/working/final_dataset/train/labels/\"\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = YOLODataset(image_dir=train_images_dir, label_dir=train_labels_dir, transform=transform)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: x)\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded with {len(train_dataset)} images in tensor format.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-tWW_NDZN_4k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display a Sample Tensor**"
      ],
      "metadata": {
        "id": "9Wr93dMcN_4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a sample from the dataset\n",
        "image_tensor, bbox_tensor = train_dataset[0]\n",
        "\n",
        "# Convert image tensor to numpy for visualization\n",
        "image_np = image_tensor.permute(1, 2, 0).numpy()\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(image_np)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Sample Tensor Image (Shape: {image_tensor.shape})\")\n",
        "plt.show()\n",
        "\n",
        "# Print the bounding box tensor\n",
        "print(\"Bounding Box Tensor:\", bbox_tensor)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "_QQzrW2yN_4l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what you had to learn to convert your data into tensors\n",
        "\n",
        "\n",
        "**Why Convert Data into Tensors?**\n",
        "\n",
        "Deep learning models, especially those built with PyTorch, do not process raw image files directly. Instead, they require numerical data in tensor format.\n",
        "For this project, I had to convert self-driving car images and YOLO object detection labels into tensors, making them compatible with deep learning models.\n",
        "\n",
        "**Understanding Tensors in PyTorch**\n",
        "\n",
        "A tensor is a multi-dimensional numerical structure similar to an array but optimized for GPU acceleration.\n",
        "\n",
        "Types of Tensors Used in This Project\n",
        "‚úÖ Image Tensors ‚Üí Represent pixel values in the format (C, H, W)\n",
        "‚úÖ Bounding Box Tensors ‚Üí Store object location in the format (class_id, x_center, y_center, width, height)\n",
        "\n",
        "\n",
        "\n",
        "**Steps to Convert Data into Tensors**\n",
        "\n",
        "1Ô∏è‚É£ Load & Transform Images:\n",
        "\n",
        "Used OpenCV (cv2) to load images and convert from BGR to RGB.\n",
        "Resized images to 640x640 and converted them to PyTorch tensors using torchvision.transforms.\n",
        "2Ô∏è‚É£ Convert YOLO Annotations to Tensors:\n",
        "\n",
        "Read YOLO .txt labels (<class_id> <x_center> <y_center> <width> <height>).\n",
        "Converted bounding boxes into PyTorch tensors and normalized coordinates (0-1 range).\n",
        "3Ô∏è‚É£ Create a PyTorch Dataset Class:\n",
        "\n",
        "Built a custom YOLODataset class to dynamically load images & labels.\n",
        "Applied transformations and returned image & bounding box tensors for training.\n",
        "‚úÖ Handled challenges like missing annotations, tensor shape mismatches, and bounding box normalization.\n",
        "\n",
        "\n",
        "By completing this step, I gained hands-on experience in:\n",
        "How deep learning models process tensor data instead of raw files.\n",
        "Using OpenCV, PyTorch, and torchvision to load & transform images.\n",
        "Efficiently handling YOLO bounding box annotations as tensors.\n",
        "Building a structured PyTorch dataset for deep learning training.\n"
      ],
      "metadata": {
        "id": "eek7AbFqN_4l"
      }
    }
  ]
}